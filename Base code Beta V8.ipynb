{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import numpy as np\n",
    "import sys\n",
    "from numpy.linalg import norm #get vector normalization\n",
    "from scipy import optimize#OPTIMIZE DAT SHET\n",
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline\n",
    "\n",
    "amount = input(\"How fine should the training data be? (higher the number the finer the data): \");\n",
    "HoSleep = input(\"How many hours did you sleep?: \");\n",
    "HoStudy = input(\"How many hours did you study?: \");\n",
    "\n",
    "try:\n",
    "    amount = int(amount);\n",
    "except:\n",
    "    amount = 100;\n",
    "\n",
    "#Supervised regression method, sup = inp & out, reg = predic of num\n",
    "X = np.array(([3, 5], [5, 1], [10, 2]), dtype=float);#[hrs sleep, hrs study]\n",
    "y = np.array(([75], [82], [93]), dtype=float);#[score]\n",
    "\n",
    "print(\"Initial X\",X);\n",
    "print(\"Initial Y\",y);\n",
    "print();\n",
    "\n",
    "#Need to account for unit diff. Inp is in hrs, out is in grade num\n",
    "#Will scale data\n",
    "#Data is positive, so will divide by max val of each var\n",
    "X = X/np.amax(X, axis = 0);#Compare to the highest number available\n",
    "y = y/100;#We know that highest score possible is 100\n",
    "#This network must follow the format of two inputs one output\n",
    "#Perspective because it is looking at everything as compared to the largest sample given, and makes a percent thereof\n",
    "print(\"Perspective X\",X);\n",
    "print(\"Perspective Y\",y);\n",
    "#In between the inp and out is a hidden network, consisting of neurons\n",
    "\"\"\"Things that connect neurons are synapses,\n",
    "    synapses take input values, and multiply them by a weight before\n",
    "    passing them on. Neurons also take multiple inputs, from\n",
    "    all their synapses, and apply an activation function.\"\"\"\n",
    "print();\n",
    "\n",
    "#building NN as a class\n",
    "class Neural_Network(object):#Make class\n",
    "    def __init__(self):#Init method\n",
    "        #use self.\"variable\" to make it class accessible\n",
    "        #Define HyperParameters\n",
    "        self.inputLayerDepth = 2;#Two inputs\n",
    "        self.outputLayerDepth = 1;#One output\n",
    "        self.hiddenLayerDepth = 3;#Three neurons to calculate with\n",
    "        self.epsilon = 1e-4;\n",
    "        \n",
    "        #Weights (Parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerDepth, \\\n",
    "                                self.hiddenLayerDepth);\n",
    "        self.W2 = np.random.randn(self.hiddenLayerDepth, \\\n",
    "                                 self.outputLayerDepth);\n",
    "    \n",
    "    def forward(self, X):#Use matrices to input multiple vals at once\n",
    "        #Propogate inputs through network\n",
    "        #Matrix multiplication to speed things up\n",
    "        #Input matrix = X, weight matrix = w(1), out matrix = Z(2)\n",
    "        #Z(2) is 3 by 3, one row per example, one col per hidden unit\n",
    "        #Func: Z(2) = Xw(1)\n",
    "        #a(2) = f(z(2))\n",
    "        self.z2 = np.dot(X, self.W1);\n",
    "        self.a2 = self.sigmoid(self.z2);\n",
    "        self.z3 = np.dot(self.a2, self.W2);\n",
    "        yHat = self.sigmoid(self.z3);\n",
    "        return yHat\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function\n",
    "        return 1/(1+np.exp(-z));\n",
    "    \n",
    "    def sigmoidPrime(self, z):#Derivation of the Sigmoid Function\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2);\n",
    "    \n",
    "    def cFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "    \n",
    "    def cFPrime(self, X, y):#Get derivs with context of W1 and W2\n",
    "        self.yHat = self.forward(X);\n",
    "        \n",
    "        #Delta of z3 data calc\n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3));\n",
    "        dW2 = np.dot(self.a2.T, delta3);\n",
    "        \n",
    "        #Delta of z2 data calc, among everything else I think\n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2);\n",
    "        dW1 = np.dot(X.T, delta2);\n",
    "        \n",
    "        return dW1, dW2;\n",
    "    \n",
    "    def getParams(self):\n",
    "        #GET W1 and W2 rolled into a vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()));\n",
    "        return params;\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #SET W1 and W2 using a single parameter vector:\n",
    "        W1_start = 0;\n",
    "        W1_end = self.hiddenLayerDepth*self.inputLayerDepth;\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], \\\n",
    "                            (self.inputLayerDepth, self.hiddenLayerDepth));\n",
    "        W2_end = W1_end+self.hiddenLayerDepth*self.outputLayerDepth;\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], \\\n",
    "                            (self.hiddenLayerDepth, self.outputLayerDepth));\n",
    "        \n",
    "    def calcGradients(self, X, y):\n",
    "        dW1, dW2 = self.cFPrime(X, y);\n",
    "        return np.concatenate((dW1.ravel(), dW2.ravel()));\n",
    "    \n",
    "    ### EXPERIMENTAL CODE ###\n",
    "    def gradNormalizer1(self, x):\n",
    "        return x**2;\n",
    "    \n",
    "    def gradNormalizer2(self, N, X, y):\n",
    "        paramsInitial = N.getParams();\n",
    "        numgrad = np.zeros(paramsInitial.shape);\n",
    "        perturb = np.zeros(paramsInitial.shape);\n",
    "        e = 1e-4;\n",
    "        \n",
    "        for a in range(len(paramsInitial)):\n",
    "            #Setting pert vector\n",
    "            perturb[a] = e;\n",
    "            N.setParams(paramsInitial+perturb);\n",
    "            loss2 = N.cFunction(X, y);\n",
    "            \n",
    "            N.setParams(paramsInitial-perturb);\n",
    "            loss1 = N.cFunction(X, y);\n",
    "            \n",
    "            #Compute slope between values:\n",
    "            numgrad[a] = (loss2-loss1)/(2*e);\n",
    "            \n",
    "            #Return to zero\n",
    "            perturb[a] = 0;\n",
    "            \n",
    "        #Reset parameters:\n",
    "        N.setParams(paramsInitial);\n",
    "        return numgrad;\n",
    "    \n",
    "    def gradNormalizer3(self, val):\n",
    "        numericGradient = (self.gradNormalizer1(val+self.epsilon)-self.gradNormalizer1(val-self.epsilon))/(2*self.epsilon);\n",
    "        return numericGradient;\n",
    "        \n",
    "\n",
    "NN = Neural_Network();#Construct NN shortcut\n",
    "yHat = NN.forward(X);#Parse yHat data\n",
    "cost1 = NN.cFunction(X, y);#Parse cost step one\n",
    "dW1, dW2 = NN.cFPrime(X, y);#Zip Values of delta calcs\n",
    "print(\"yHat (rough predict of change of values):\", yHat);#Let's see what this crap is\n",
    "print(\"Y (rough predict)\",y);\n",
    "print();#Organization of prints\n",
    "print(\"delta W1:\", dW1);#Derivitive of J in respect to derivitive of W (Find the value of the slope at all values quickly) for 1 and 2\n",
    "print(\"delta W2:\", dW2);\n",
    "\n",
    "scalar = 3;#Scale the calculation gradient (I believe)\n",
    "NN.W1 = NN.W1+scalar+dW1;\n",
    "NN.W2 = NN.W2+scalar+dW2;\n",
    "cost2 = NN.cFunction(X, y);#Cost val 2\n",
    "\n",
    "print();#Organization of prints\n",
    "print(\"Cost 1&2:\",cost1, cost2);#Print it\n",
    "\n",
    "dW1, dW2 = NN.cFPrime(X, y);#Redoe delta calcs in zip\n",
    "NN.W1 = NN.W1-scalar+dW1;\n",
    "NN.W2 = NN.W2-scalar+dW2;\n",
    "cost3 = NN.cFunction(X, y);\n",
    "\n",
    "print();#Organization of prints\n",
    "print(\"Cost 2&3:\",cost2, cost3);#Print it\n",
    "print();\n",
    "\n",
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        #Scope out to see NN\n",
    "        self.N = N;\n",
    "        \n",
    "    def cFWrapper(self, params, X, y):#Wrap values\n",
    "        self.N.setParams(params);\n",
    "        cost = self.N.cFunction(X, y);\n",
    "        grad = self.N.calcGradients(X, y);\n",
    "        return cost, grad;\n",
    "    \n",
    "    def cbF(self, params):\n",
    "        self.N.setParams(params);\n",
    "        self.J.append(self.N.cFunction(self.X, self.y));\n",
    "        \n",
    "    \n",
    "    def train(self, X, y):\n",
    "        \n",
    "        #Making internal variable for callback func\n",
    "        self.X = X;\n",
    "        self.y = y;\n",
    "        #Now store the costs:\n",
    "        self.J = [];\n",
    "        \n",
    "        params0 = self.N.getParams();\n",
    "        \n",
    "        options = {'maxiter': 200, 'disp': True};\n",
    "        _res = optimize.minimize(self.cFWrapper, params0, \\\n",
    "                                 jac = True, method='BFGS', \\\n",
    "                                 args = (X, y), options = options, \\\n",
    "                                 callback = self.cbF);\n",
    "        #Replace random parameters with trained parameters\n",
    "        self.N.setParams(_res.x);\n",
    "        self.optResults = _res;\n",
    "\n",
    "graph1 = plot.figure(1);\n",
    "tValues = np.arange(-5, 5, 0.01);\n",
    "plot.plot(tValues, NN.sigmoid(tValues), linewidth = 2);\n",
    "plot.plot(tValues, NN.sigmoidPrime(tValues), linewidth = 2);\n",
    "plot.grid(1);\n",
    "plot.legend(['sigmoid', 'sigmoidPrime']);\n",
    "T = trainer(NN);\n",
    "T.train(X, y);\n",
    "graph2 = plot.figure(2);\n",
    "plot.plot(T.J, linewidth = 2);\n",
    "plot.grid(1);\n",
    "plot.ylabel('Cost');\n",
    "plot.xlabel('Iterations');\n",
    "#Test network for combos\n",
    "hoursSleep = np.linspace(0, 10, amount);\n",
    "hoursStudy = np.linspace(0, 5, amount);\n",
    "#print(hoursSleep, \"\\n\\n\\n\", hoursStudy);\n",
    "#print();\n",
    "#Normalizing data:\n",
    "hoursSleepNorm = hoursSleep/10.;\n",
    "hoursStudyNorm = hoursStudy/5.\n",
    "\n",
    "#Flatten Data:\n",
    "a, b = np.meshgrid(hoursSleepNorm, hoursStudyNorm);\n",
    "\n",
    "#Put it in the Matrix\n",
    "allInputs = np.zeros((a.size, 2));\n",
    "allInputs[:, 0] = a.ravel();\n",
    "allInputs[:, 1] = b.ravel();\n",
    "allOut = NN.forward(allInputs);\n",
    "print(allOut);\n",
    "print();\n",
    "print();\n",
    "#print(NN.forward(X));\n",
    "\n",
    "graph3 = plot.figure(3);\n",
    "yy = np.dot(hoursStudy.reshape(amount, 1), np.ones((1, amount)));\n",
    "xx = np.dot(hoursSleep.reshape(amount, 1), np.ones((1, amount))).T;\n",
    "\n",
    "CS = plot.contour(xx, yy, 100*allOut.reshape(amount, amount));\n",
    "plot.clabel(CS, inline = 1, fontsize = 10);\n",
    "plot.xlabel(\"Hours Slept\");\n",
    "plot.ylabel(\"Hours Studied\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
